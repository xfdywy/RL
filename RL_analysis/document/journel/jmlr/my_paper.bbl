\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahdanau et~al.(2016)Bahdanau, Brakel, Xu, Goyal, Lowe, Pineau,
  Courville, and Bengio]{bahdanau2016actor}
Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle
  Pineau, Aaron Courville, and Yoshua Bengio.
\newblock An actor-critic algorithm for sequence prediction.
\newblock \emph{arXiv preprint arXiv:1607.07086}, 2016.

\bibitem[Bhatnagar et~al.(2009)Bhatnagar, Precup, Silver, Sutton, Maei, and
  Szepesv{\'a}ri]{bhatnagar2009convergent}
Shalabh Bhatnagar, Doina Precup, David Silver, Richard~S Sutton, Hamid~R Maei,
  and Csaba Szepesv{\'a}ri.
\newblock Convergent temporal-difference learning with arbitrary smooth
  function approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1204--1212, 2009.

\bibitem[Dann et~al.(2014)Dann, Neumann, and Peters]{dann2014policy}
Christoph Dann, Gerhard Neumann, and Jan Peters.
\newblock Policy evaluation with temporal differences: a survey and comparison.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 809--883, 2014.

\bibitem[Duchi et~al.(2012)Duchi, Agarwal, Johansson, and
  Jordan]{duchi2012ergodic}
John~C Duchi, Alekh Agarwal, Mikael Johansson, and Michael~I Jordan.
\newblock Ergodic mirror descent.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (4):\penalty0
  1549--1578, 2012.

\bibitem[Durrett(2016)]{durrett2016poisson}
Richard Durrett.
\newblock Poisson processes.
\newblock In \emph{Essentials of Stochastic Processes}, pages 95--124.
  Springer, 2016.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{kober2013reinforcement}
Jens Kober, J~Andrew Bagnell, and Jan Peters.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 32\penalty0
  (11):\penalty0 1238--1274, 2013.

\bibitem[Lazaric et~al.(2012)Lazaric, Ghavamzadeh, and
  Munos]{lazaric2012finite}
Alessandro Lazaric, Mohammad Ghavamzadeh, and R{\'e}mi Munos.
\newblock Finite-sample analysis of least-squares policy iteration.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Oct):\penalty0 3041--3074, 2012.

\bibitem[Levin et~al.(2009)Levin, Peres, and Wilmer]{levin2009markov}
David~Asher Levin, Yuval Peres, and Elizabeth~Lee Wilmer.
\newblock \emph{Markov chains and mixing times}.
\newblock American Mathematical Soc., 2009.

\bibitem[Lin(1993)]{lin1993reinforcement}
Long-Ji Lin.
\newblock \emph{Reinforcement learning for robots using neural networks}.
\newblock PhD thesis, Fujitsu Laboratories Ltd, 1993.

\bibitem[Liu et~al.(2015)Liu, Liu, Ghavamzadeh, Mahadevan, and
  Petrik]{liu2015finite}
Bo~Liu, Ji~Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik.
\newblock Finite-sample analysis of proximal gradient td algorithms.
\newblock In \emph{UAI}, pages 504--513. Citeseer, 2015.

\bibitem[Maei(2011)]{maei2011gradient}
Hamid~Reza Maei.
\newblock \emph{Gradient temporal-difference learning algorithms}.
\newblock PhD thesis, University of Alberta, 2011.

\bibitem[Meyn and Tweedie(2012)]{meyn2012markov}
Sean~P Meyn and Richard~L Tweedie.
\newblock \emph{Markov chains and stochastic stability}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Nemirovski(2004)]{nemirovski2004prox}
Arkadi Nemirovski.
\newblock Prox-method with rate of convergence o (1/t) for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (1):\penalty0
  229--251, 2004.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning (ICML-14)}, pages 387--395, 2014.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Sutton(1988)]{sutton1988learning}
Richard~S Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton and Barto(1998)]{sutton1998reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press Cambridge, 1998.

\bibitem[Sutton et~al.(2009{\natexlab{a}})Sutton, Maei, and
  Szepesv{\'a}ri]{sutton2009convergent}
Richard~S Sutton, Hamid~R Maei, and Csaba Szepesv{\'a}ri.
\newblock A convergent $ o (n) $ temporal-difference algorithm for off-policy
  learning with linear function approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1609--1616, 2009{\natexlab{a}}.

\bibitem[Sutton et~al.(2009{\natexlab{b}})Sutton, Maei, Precup, Bhatnagar,
  Silver, Szepesv{\'a}ri, and Wiewiora]{sutton2009fast}
Richard~S Sutton, Hamid~Reza Maei, Doina Precup, Shalabh Bhatnagar, David
  Silver, Csaba Szepesv{\'a}ri, and Eric Wiewiora.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 993--1000. ACM, 2009{\natexlab{b}}.

\bibitem[Tagorti and Scherrer(2015)]{tagorti2015rate}
Manel Tagorti and Bruno Scherrer.
\newblock On the rate of convergence and error bounds for lstd ( $ lambda $).
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML-15)}, pages 1521--1529, 2015.

\bibitem[Tsitsiklis et~al.(1997)Tsitsiklis, Van~Roy,
  et~al.]{tsitsiklis1997analysis}
John~N Tsitsiklis, Benjamin Van~Roy, et~al.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{IEEE transactions on automatic control}, 42\penalty0
  (5):\penalty0 674--690, 1997.

\bibitem[Yu(2015)]{yu2015convergence}
H~Yu.
\newblock On convergence of emphatic temporal-difference learning.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory},
  pages 1724--1751, 2015.

\end{thebibliography}
