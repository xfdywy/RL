%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Template for a LaTex article in English.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}

% AMS packages:
\usepackage{amsmath, amsthm, amsfonts}
\usepackage{hyperref}

% Theorems
%-----------------------------------------------------------------
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}

% Shortcuts.
% One can define new commands to shorten frequently used
% constructions. As an example, this defines the R and Z used
% for the real and integer numbers.
%-----------------------------------------------------------------
\def\RR{\mathbb{T}}
\def\ZZ{\mathbb{Z}}

% Similarly, one can define commands that take arguments. In this
% example we define a command for the absolute value.
% -----------------------------------------------------------------
\newcommand{\abs}[1]{\left\vert#1\right\vert}

% Operators
% New operators must defined as such to have them typeset
% correctly. As an example we define the Jacobian:
% -----------------------------------------------------------------
\DeclareMathOperator{\Jac}{Bc}

%-----------------------------------------------------------------
\title{Reinformcement Learning Theory Learning}
\author{Yue Wang}

\begin{document}
	\maketitle
	
	\abstract{This is a simple template for an article  written in \LaTeX.}
	
	\section{Introduction}
	\label{introduction}
	
	RL theory is very long history and these days
	
	\begin{itemize}
		\item Bullet point one
		\item Bullet point two
	\end{itemize}
	
	\begin{enumerate}
		\item Numbered list item one
		\item Numbered list item two
	\end{enumerate}
	
	\section{Notation and Formatting}
	\label{notation}
	some notations: 
	
	\begin{center}
		\begin{tabular}{ |c  c| } 
			\hline
			$ \mathcal{S} $ &    the state space \\ 
			$ S_t $ &    the state at time t, stochastic  \\ 
			$ s_t $ &    the state at time t, actual  \\ 
			$ s $ & the state, actual\\
			
			
			$ R_t $ & the reward at time t, stochastic\\ 
			$ r_t $ & the reward at time t, actual\\
			$ r $ & the reward, actual\\
			
			
			
			
			
			\hline
		\end{tabular}
	\end{center}
	
	
	
	
	
	
	\section{RL algorithms}
	\label{algorithms}
	
	\section{RL convergence theory}
	\label{convergence}
	
	\subsection{counterexample}
	
	In \cite[chap 11.3]{sutton2011reinforcement} the authors give  an intuitive conclusion about when these algorithms will divergence :
	\begin{quotation}
		the danger of instability and divergence arises whenever we combine  three things:
		\begin{enumerate}
			\item training on a distribution of trainsition other than that naturally generated by the process whose expectation is being estimated(e.g. off-policy learning)
			\item scalable function approximation (e.g. li)
		\end{enumerate}
		
	\end{quotation}
	
	\subsubsection{counterexample1}
	In \cite{bertsekas1995counterexample} 
	% Bibliography
	%-----------------------------------------------------------------

		\bibliographystyle{ieeetr}
		\bibliography{../ref/sample,../ref/counterexample}
	
	
\end{document}