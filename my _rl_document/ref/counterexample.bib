@article{Thrun1993,
abstract = {Reinforcement learning techniques address the problem of learning to select actions in unknown, dynamic environments. It is widely acknowledged that to be of use in complex domains, reinforcement learning techniques must be combined with generalizing function approximation methods such as artificial neural networks. Little, however, is understood about the theoretical properties of such combinations, and many researchers have encountered failures in practice. In this paper we identify a prime source of such failures---namely, a systematic overestimation of utility values. Using Watkins' Q-Learning [18] as an example, we give a theoretical account of the phenomenon, deriving conditions under which one may expected it to cause learning to fail. Employing some of the most popular function approximators, we present experimental results which support the theoretical findings.},
annote = {q learning},
author = {Thrun, Sebastian and Schwartz, Anton},
file = {:D$\backslash$:/paper/rltheory/convergencerate/Issues in Using Function Approximation for Reinforcement Learning.pdf:pdf},
journal = {Proceedings of the 1993 Connectionist Models Summer School},
mendeley-groups = {RL,RL/counterexample},
title = {{Issues in using function approximation for reinforcement learning}},
year = {1993}
}
@article{Tsitsiklis1996,
author = {Tsitsiklis, JN and Roy, B Van},
file = {:C$\backslash$:/Users/v-yuewng/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsitsiklis, Roy - 1996 - Feature-based methods for large scale dynamic programming.pdf:pdf},
journal = {Machine Learning},
mendeley-groups = {RL/counterexample},
title = {{Feature-based methods for large scale dynamic programming}},
url = {http://link.springer.com/article/10.1023/A:1018008221616},
year = {1996}
}
@article{Dimitri,
author = {Dimitri, P},
file = {:C$\backslash$:/Users/v-yuewng/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dimitri - Unknown - A Counterexample to Temporal Differences Learning.pdf:pdf},
journal = {pdfs.semanticscholar.org},
mendeley-groups = {RL/counterexample},
title = {{A Counterexample to Temporal Differences Learning}},
url = {https://pdfs.semanticscholar.org/e7b9/9e6a02ef75adc581c17e1e0c6038f48a3e47.pdf}
}
@article{Boyan1995,
author = {Boyan, J and Moore, AW},
file = {:C$\backslash$:/Users/v-yuewng/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boyan, Moore - 1995 - Generalization in reinforcement learning Safely approximating the value function.pdf:pdf},
journal = {Advances in neural information processing},
mendeley-groups = {RL/counterexample},
title = {{Generalization in reinforcement learning: Safely approximating the value function}},
url = {https://www.google.com/books?hl=zh-CN{\&}lr={\&}id=M9WuI6tiqRcC{\&}oi=fnd{\&}pg=PA369{\&}dq=Generalization+in+Reinforcement+Learning:+Safely+Approxi-+mating+the+Value+Function{\&}ots=GubxdY1wND{\&}sig=gw{\_}rbDmYXhyHLcVxo9xnnk0NW8o},
year = {1995}
}

@article{Baird,
abstract = {A number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables. It is shown, however, that these algorithms can easily become unstable when implemented directly with a general function-approximation system, such as a sigmoidal multilayer perceptron, a radial-basis-function system, a memory-based learning system, or even a linear function-approximation system. A new class of algorithms, residual gradient algorithms, is proposed, which perform gradient descent on the mean squared Bellman residual, guaranteeing convergence. It is shown, however, that they may learn very slowly in some cases. A larger class of algorithms, residual algorithms, is proposed that has the guaranteed convergence of the residual gradient algorithms, yet can retain the fast learning speed of direct algorithms. In fact, both direct and residual gradient algorithms are shown to be special cases of residual algorithms, and it is shown that residual algorithms can combine the advantages of each approach. The direct, residual gradient, and residual forms of value iteration, Q-learning, and advantage learning are all presented. Theoretical analysis is given explaining the properties these algorithms have, and simulation results are given that demonstrate these properties.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Baird, Leemon},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/v-yuewng/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baird - Unknown - Residual Algorithms Reinforcement Learning with Function Approximation(2).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Icml},
keywords = {dblp},
mendeley-groups = {RL/other algorithms,RL/counterexample},
pages = {30--37},
pmid = {25246403},
title = {{Residual Algorithms: Reinforcement Learning with Function Approximation.}},
url = {http://kirk.usafa.af.mil/{~}baird},
year = {1995}
}

@article{Tsitsiklis1996,
abstract = {We develop a methodological framework and present a few different ways in which dynamic programming and compact representations can be combined to solve large scale stochastic control problems. In particular, we develop algorithms that employ two types of feature-based compact representations; that is, representations that involve feature extraction and a relatively simple approximation architecture. We prove the convergence of these algorithms and provide bounds on the approximation error. As an example, one of these algorithms is used to generate a strategy for the game of Tetris. Furthermore, we provide a counter-example illustrating the difficulties of integrating compact representations with dynamic programming, which exemplifies the shortcomings of certain simple approaches.},
author = {Tsitsiklis, John N. and {Van Roy}, Benjamin},
doi = {10.1007/BF00114724},
file = {:C$\backslash$:/Users/v-yuewng/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsitsiklis, Roy - 1996 - Feature-based methods for large scale dynamic programming.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {compact representation,curse of dimensionality,dynamic programming,features,function approximation,neuro-dynamic programming,proximation,reinforcement learning},
mendeley-groups = {RL/counterexample},
number = {1-3},
pages = {59--94},
title = {{Feature-based methods for large scale dynamic programming}},
url = {http://link.springer.com/article/10.1023/A:1018008221616},
volume = {22},
year = {1996}
}
