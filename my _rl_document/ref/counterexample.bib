@article{Thrun1993,
abstract = {Reinforcement learning techniques address the problem of learning to select actions in unknown, dynamic environments. It is widely acknowledged that to be of use in complex domains, reinforcement learning techniques must be combined with generalizing function approximation methods such as artificial neural networks. Little, however, is understood about the theoretical properties of such combinations, and many researchers have encountered failures in practice. In this paper we identify a prime source of such failures---namely, a systematic overestimation of utility values. Using Watkins' Q-Learning [18] as an example, we give a theoretical account of the phenomenon, deriving conditions under which one may expected it to cause learning to fail. Employing some of the most popular function approximators, we present experimental results which support the theoretical findings.},
annote = {q learning},
author = {Thrun, Sebastian and Schwartz, Anton},
file = {:D$\backslash$:/paper/rltheory/convergencerate/Issues in Using Function Approximation for Reinforcement Learning.pdf:pdf},
journal = {Proceedings of the 1993 Connectionist Models Summer School},
mendeley-groups = {RL,RL/counterexample},
title = {{Issues in using function approximation for reinforcement learning}},
year = {1993}
}
@article{Tsitsiklis1996,
author = {Tsitsiklis, JN and Roy, B Van},
file = {:C$\backslash$:/Users/v-yuewng/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsitsiklis, Roy - 1996 - Feature-based methods for large scale dynamic programming.pdf:pdf},
journal = {Machine Learning},
mendeley-groups = {RL/counterexample},
title = {{Feature-based methods for large scale dynamic programming}},
url = {http://link.springer.com/article/10.1023/A:1018008221616},
year = {1996}
}
@article{Dimitri,
author = {Dimitri, P},
file = {:C$\backslash$:/Users/v-yuewng/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dimitri - Unknown - A Counterexample to Temporal Differences Learning.pdf:pdf},
journal = {pdfs.semanticscholar.org},
mendeley-groups = {RL/counterexample},
title = {{A Counterexample to Temporal Differences Learning}},
url = {https://pdfs.semanticscholar.org/e7b9/9e6a02ef75adc581c17e1e0c6038f48a3e47.pdf}
}
@article{Boyan1995,
author = {Boyan, J and Moore, AW},
file = {:C$\backslash$:/Users/v-yuewng/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boyan, Moore - 1995 - Generalization in reinforcement learning Safely approximating the value function.pdf:pdf},
journal = {Advances in neural information processing},
mendeley-groups = {RL/counterexample},
title = {{Generalization in reinforcement learning: Safely approximating the value function}},
url = {https://www.google.com/books?hl=zh-CN{\&}lr={\&}id=M9WuI6tiqRcC{\&}oi=fnd{\&}pg=PA369{\&}dq=Generalization+in+Reinforcement+Learning:+Safely+Approxi-+mating+the+Value+Function{\&}ots=GubxdY1wND{\&}sig=gw{\_}rbDmYXhyHLcVxo9xnnk0NW8o},
year = {1995}
}
